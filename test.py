"""
Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ GraTech Platform
"""

import asyncio
import sys
sys.path.append('src')

from api.gateway import AIGateway


async def test_all_models():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
    
    print("=" * 60)
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù…Ù†ØµØ© GraTech AI - Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
    print("=" * 60)
    print()
    
    gateway = AIGateway()
    
    # 1. Ø¹Ø±Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©
    print("ğŸ“‹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©:")
    print("-" * 60)
    for model in gateway.list_models():
        print(f"âœ“ {model['name']}")
        print(f"  Ø§Ù„ÙˆØµÙ: {model['description']}")
        print(f"  Ø§Ù„Ù…Ø²ÙˆØ¯: {model['provider']}")
        print(f"  Ø§Ù„Ù…Ù†Ø·Ù‚Ø©: {model['region']}")
        print()
    
    # 2. Ø§Ø®ØªØ¨Ø§Ø± ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬
    test_message = "Ù…Ø±Ø­Ø¨Ø§Ù‹! Ù‚Ù„ Ù„ÙŠ Ø´ÙŠØ¦Ø§Ù‹ Ù‚ØµÙŠØ±Ø§Ù‹ Ø¹Ù† Ù†ÙØ³Ùƒ."
    
    print("\n" + "=" * 60)
    print("ğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬:")
    print("=" * 60)
    print()
    
    results = {}
    
    for model_name in gateway.config['models'].keys():
        print(f"ğŸ“¡ Ø§Ø®ØªØ¨Ø§Ø±: {model_name}")
        print("-" * 40)
        
        try:
            response = await gateway.chat(
                model=model_name,
                messages=[{'role': 'user', 'content': test_message}],
                max_tokens=100
            )
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ
            if 'choices' in response:
                text = response['choices'][0]['message']['content']
            elif 'content' in response:
                text = response['content'][0]['text']
            else:
                text = str(response)
            
            print(f"âœ… Ù†Ø¬Ø­!")
            print(f"Ø§Ù„Ø±Ø¯: {text[:150]}...")
            results[model_name] = "âœ… ÙŠØ¹Ù…Ù„"
            
        except Exception as e:
            print(f"âŒ ÙØ´Ù„!")
            print(f"Ø§Ù„Ø®Ø·Ø£: {str(e)}")
            results[model_name] = f"âŒ Ø®Ø·Ø£: {str(e)[:50]}"
        
        print()
    
    # 3. Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print("\n" + "=" * 60)
    print("ğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:")
    print("=" * 60)
    print()
    
    for model, status in results.items():
        print(f"{model:20} â†’ {status}")
    
    # 4. Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­
    success_count = sum(1 for s in results.values() if "âœ…" in s)
    total_count = len(results)
    success_rate = (success_count / total_count) * 100
    
    print()
    print("=" * 60)
    print(f"âœ¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {success_count}/{total_count} ({success_rate:.1f}%)")
    print("=" * 60)
    
    if success_rate == 100:
        print("\nğŸ‰ Ù…Ù…ØªØ§Ø²! Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ØªØ¹Ù…Ù„ Ø¨Ù†Ø¬Ø§Ø­!")
    elif success_rate >= 60:
        print("\nâš ï¸  ØªØ­Ø°ÙŠØ±: Ø¨Ø¹Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ø§ ØªØ¹Ù…Ù„. Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙˆØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª.")
    else:
        print("\nâŒ Ø®Ø·Ø£: Ù…Ø¹Ø¸Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ø§ ØªØ¹Ù…Ù„. ØªØ­Ù‚Ù‚ Ù…Ù†:")
        print("   1. Ù…Ù„Ù .env.production ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„ØµØ­ÙŠØ­Ø©")
        print("   2. Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª")
        print("   3. ØµÙ„Ø§Ø­ÙŠØ§Øª Azure")


async def test_specific_model(model_name: str, question: str):
    """Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ø¯Ø¯ Ø¨Ø³Ø¤Ø§Ù„ Ù…Ø­Ø¯Ø¯"""
    
    print(f"\nğŸ¯ Ø§Ø®ØªØ¨Ø§Ø± {model_name} Ù…Ø¹ Ø³Ø¤Ø§Ù„ Ù…Ø®ØµØµ:")
    print("=" * 60)
    print(f"Ø§Ù„Ø³Ø¤Ø§Ù„: {question}")
    print("-" * 60)
    
    gateway = AIGateway()
    
    try:
        response = await gateway.chat(
            model=model_name,
            messages=[{'role': 'user', 'content': question}],
            temperature=0.7,
            max_tokens=500
        )
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ
        if 'choices' in response:
            text = response['choices'][0]['message']['content']
        elif 'content' in response:
            text = response['content'][0]['text']
        else:
            text = str(response)
        
        print(f"âœ… Ø§Ù„Ø±Ø¯:\n{text}")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£: {str(e)}")
        print("=" * 60)


async def interactive_test():
    """ÙˆØ¶Ø¹ ØªÙØ§Ø¹Ù„ÙŠ Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
    
    gateway = AIGateway()
    
    print("\n" + "=" * 60)
    print("ğŸ’¬ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠ - GraTech AI")
    print("=" * 60)
    print()
    print("Ø§ÙƒØªØ¨ 'exit' Ù„Ù„Ø®Ø±ÙˆØ¬ØŒ 'models' Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
    print()
    
    # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    models = list(gateway.config['models'].keys())
    print("Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©:")
    for i, model in enumerate(models, 1):
        print(f"{i}. {model}")
    
    choice = input("\nØ§Ø®ØªØ± Ø±Ù‚Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ").strip()
    
    try:
        model_name = models[int(choice) - 1]
        print(f"\nâœ“ ØªÙ… Ø§Ø®ØªÙŠØ§Ø±: {model_name}\n")
    except:
        print("âŒ Ø§Ø®ØªÙŠØ§Ø± ØºÙŠØ± ØµØ­ÙŠØ­. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ: gpt-4o")
        model_name = "gpt-4o"
    
    # Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
    messages = []
    
    while True:
        user_input = input("Ø£Ù†Øª: ").strip()
        
        if user_input.lower() == 'exit':
            print("\nğŸ‘‹ ÙˆØ¯Ø§Ø¹Ø§Ù‹!")
            break
        
        if user_input.lower() == 'models':
            print("\nØ§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©:")
            for model in gateway.list_models():
                print(f"  â€¢ {model['name']}: {model['description']}")
            print()
            continue
        
        if not user_input:
            continue
        
        messages.append({'role': 'user', 'content': user_input})
        
        try:
            print(f"\n{model_name}: ", end="", flush=True)
            
            response = await gateway.chat(
                model=model_name,
                messages=messages,
                temperature=0.7,
                max_tokens=2000
            )
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ
            if 'choices' in response:
                text = response['choices'][0]['message']['content']
            elif 'content' in response:
                text = response['content'][0]['text']
            else:
                text = str(response)
            
            print(text)
            print()
            
            messages.append({'role': 'assistant', 'content': text})
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£: {str(e)}\n")


if __name__ == '__main__':
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == 'test':
            # Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
            asyncio.run(test_all_models())
        
        elif command == 'model' and len(sys.argv) > 3:
            # Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ø¯Ø¯
            model_name = sys.argv[2]
            question = ' '.join(sys.argv[3:])
            asyncio.run(test_specific_model(model_name, question))
        
        elif command == 'interactive':
            # ÙˆØ¶Ø¹ ØªÙØ§Ø¹Ù„ÙŠ
            asyncio.run(interactive_test())
        
        else:
            print("Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:")
            print("  python test.py test                    # Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬")
            print("  python test.py model <name> <question> # Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ø¯Ø¯")
            print("  python test.py interactive             # ÙˆØ¶Ø¹ ØªÙØ§Ø¹Ù„ÙŠ")
    
    else:
        # Ø§ÙØªØ±Ø§Ø¶ÙŠØ§Ù‹: Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        asyncio.run(test_all_models())

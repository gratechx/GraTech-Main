"""
GraTech AI Gateway - Ø¨ÙˆØ§Ø¨Ø© Ù…ÙˆØ­Ø¯Ø© Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
ÙŠØ¯Ø¹Ù…: GPT-4o, GPT-4.1, Claude Opus 4.5, DeepSeek R1, O3-mini
"""

import os
import json
from typing import Dict, Optional, List
from enum import Enum
import httpx
from dotenv import load_dotenv

load_dotenv()

class ModelProvider(Enum):
    AZURE_OPENAI = "azure-openai"
    AZURE_FOUNDRY = "azure-foundry"

class AIGateway:
    """Ø¨ÙˆØ§Ø¨Ø© Ù…ÙˆØ­Ø¯Ø© Ù„Ù„Ø§ØªØµØ§Ù„ Ø¨Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
    
    def __init__(self):
        # ØªØ­Ù…ÙŠÙ„ ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        with open('config/models.json', 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        # Ù…ÙØ§ØªÙŠØ­ API
        self.openai_key = os.getenv('AZURE_OPENAI_API_KEY')
        self.foundry_key = os.getenv('AZURE_FOUNDRY_API_KEY')
        
    async def chat(
        self,
        model: str,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 4000,
        **kwargs
    ) -> Dict:
        """
        Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø¥Ù„Ù‰ Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬
        
        Args:
            model: Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (gpt-4o, claude-opus-4-5, etc.)
            messages: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
            temperature: Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ (0-1)
            max_tokens: Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø±Ù…ÙˆØ²
            
        Returns:
            Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        """
        
        if model not in self.config['models']:
            raise ValueError(f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ '{model}' ØºÙŠØ± Ù…ØªØ§Ø­. Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©: {list(self.config['models'].keys())}")
        
        model_config = self.config['models'][model]
        provider = model_config['provider']
        
        if provider == ModelProvider.AZURE_OPENAI.value:
            return await self._call_azure_openai(model_config, messages, temperature, max_tokens, **kwargs)
        elif provider == ModelProvider.AZURE_FOUNDRY.value:
            return await self._call_azure_foundry(model_config, messages, temperature, max_tokens, **kwargs)
        else:
            raise ValueError(f"Ù…Ø²ÙˆØ¯ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {provider}")
    
    async def _call_azure_openai(
        self,
        config: Dict,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int,
        **kwargs
    ) -> Dict:
        """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Azure OpenAI"""
        
        url = f"{config['endpoint']}openai/deployments/{config['deployment']}/chat/completions?api-version={config['apiVersion']}"
        
        headers = {
            'api-key': self.openai_key,
            'Content-Type': 'application/json'
        }
        
        payload = {
            'messages': messages,
            'temperature': temperature,
            'max_tokens': max_tokens,
            **kwargs
        }
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(url, json=payload, headers=headers)
            response.raise_for_status()
            return response.json()
    
    async def _call_azure_foundry(
        self,
        config: Dict,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int,
        **kwargs
    ) -> Dict:
        """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Azure AI Foundry (Claude, DeepSeek)"""
        
        # Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ù† Anthropic Ù†Ø³ØªØ®Ø¯Ù… Messages API
        if 'claude' in config['deployment'].lower():
            url = f"{config['endpoint']}/models/{config['deployment']}/chat/completions?api-version={config['apiVersion']}"
        else:
            url = f"{config['endpoint']}/chat/completions?api-version={config['apiVersion']}"
        
        headers = {
            'api-key': self.foundry_key,
            'Content-Type': 'application/json'
        }
        
        payload = {
            'model': config['deployment'],
            'messages': messages,
            'temperature': temperature,
            'max_tokens': max_tokens,
            **kwargs
        }
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(url, json=payload, headers=headers)
            
            if response.status_code != 200:
                error_detail = response.text
                raise Exception(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ {config['deployment']}: {response.status_code} - {error_detail}")
            
            return response.json()
    
    def list_models(self) -> List[Dict]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        return [
            {
                'name': name,
                'description': info['description'],
                'provider': info['provider'],
                'region': info['region']
            }
            for name, info in self.config['models'].items()
        ]
    
    async def test_connection(self, model: str) -> bool:
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ÙŠÙ†"""
        try:
            result = await self.chat(
                model=model,
                messages=[{'role': 'user', 'content': 'Ù…Ø±Ø­Ø¨Ø§Ù‹'}],
                max_tokens=50
            )
            return True
        except Exception as e:
            print(f"ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ {model}: {e}")
            return False


# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
async def main():
    gateway = AIGateway()
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬...\n")
    
    for model_name in gateway.config['models'].keys():
        print(f"ğŸ“¡ Ø§Ø®ØªØ¨Ø§Ø± {model_name}...")
        success = await gateway.test_connection(model_name)
        status = "âœ… ÙŠØ¹Ù…Ù„" if success else "âŒ ÙØ´Ù„"
        print(f"   {status}\n")
    
    # Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ù…Ø­Ø§Ø¯Ø«Ø©
    print("\nğŸ’¬ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Claude Opus:")
    response = await gateway.chat(
        model='claude-opus-4-5',
        messages=[
            {'role': 'user', 'content': 'Ø§Ø´Ø±Ø­ Ù„ÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¨Ø³ÙŠØ·Ø©'}
        ]
    )
    print(response)


if __name__ == '__main__':
    import asyncio
    asyncio.run(main())
